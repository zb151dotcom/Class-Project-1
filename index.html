<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>AI & Privacy Law — Class Resource</title>
  <meta name="description" content="Class resource: full citation, structured summary, doctrinal connections, critical analysis, discussion question, and slides." />
  <style>
    :root{
      --bg:#0b0f14; --card:#111823; --text:#e8eef7; --muted:#b7c3d6; --accent:#6aa6ff;
      --line:rgba(255,255,255,.12); --shadow: 0 10px 30px rgba(0,0,0,.35); --radius:16px; --max: 980px;
    }
    *{box-sizing:border-box}
    body{
      margin:0; font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial;
      background: radial-gradient(1200px 900px at 20% 0%, rgba(106,166,255,.18), transparent 55%),
                  radial-gradient(900px 700px at 90% 10%, rgba(138,255,203,.10), transparent 55%),
                  var(--bg);
      color:var(--text); line-height:1.65;
    }
    a{color:var(--accent); text-decoration:none}
    a:hover{text-decoration:underline}
    header{
      position: sticky; top: 0; backdrop-filter: blur(10px);
      background: rgba(11,15,20,.75); border-bottom: 1px solid var(--line); z-index: 10;
    }
    .nav{
      max-width: var(--max); margin: 0 auto; padding: 14px 18px;
      display:flex; align-items:center; gap: 14px; justify-content: space-between;
    }
    .brand{display:flex; flex-direction:column; gap:2px}
    .brand strong{font-size: 15px}
    .brand span{font-size: 12px; color: var(--muted)}
    nav{display:flex; flex-wrap:wrap; gap: 10px; justify-content:flex-end}
    nav a{
      font-size: 13px; padding: 7px 10px; border: 1px solid transparent; border-radius: 999px; color: var(--muted);
    }
    nav a:hover{border-color: var(--line); color: var(--text); text-decoration:none}
    main{max-width: var(--max); margin: 0 auto; padding: 26px 18px 60px}
    .hero{
      margin-top: 14px; padding: 22px;
      background: linear-gradient(180deg, rgba(255,255,255,.06), rgba(255,255,255,.02));
      border: 1px solid var(--line); border-radius: var(--radius); box-shadow: var(--shadow);
    }
    .hero h1{margin: 0 0 6px 0; font-size: 26px; line-height:1.2}
    .meta{color: var(--muted); font-size: 13px; display:flex; flex-wrap:wrap; gap: 10px 16px; margin-top: 10px}
    .pill{padding: 4px 10px; border-radius: 999px; border: 1px solid var(--line); background: rgba(255,255,255,.03)}
    .grid{display:grid; grid-template-columns: 1fr; gap: 16px; margin-top: 16px}
    @media (min-width: 900px){ .grid{grid-template-columns: 1fr 1fr} }
    section.card{
      padding: 18px; background: var(--card); border: 1px solid var(--line);
      border-radius: var(--radius); box-shadow: var(--shadow);
    }
    section.card h2{margin: 0 0 10px 0; font-size: 18px}
    .subtle{color: var(--muted); font-size: 13px; margin-top: -6px; margin-bottom: 10px}
    .divider{height:1px; background: var(--line); margin: 12px 0}
    ul{margin: 10px 0 0 18px}
    li{margin: 6px 0}
    .callout{
      padding: 12px 14px; border-left: 3px solid var(--accent);
      background: rgba(106,166,255,.08); border-radius: 10px; margin-top: 10px;
    }
    .label{font-size: 12px; letter-spacing: .08em; text-transform: uppercase; color: var(--muted)}
    .citation{
      font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      font-size: 13px; background: rgba(255,255,255,.03); border: 1px solid var(--line);
      padding: 12px; border-radius: 12px; overflow-x:auto;
    }
    .slides{aspect-ratio: 16/9; width: 100%; border: 1px solid var(--line); border-radius: 14px; overflow:hidden; background: rgba(255,255,255,.02)}
    iframe{border:0; width:100%; height:100%}
    footer{max-width: var(--max); margin: 0 auto; padding: 18px; color: var(--muted); font-size: 12px; border-top: 1px solid var(--line)}
  </style>
</head>

<body>
<header>
  <div class="nav">
    <div class="brand">
      <strong>AI and the Law, Spring 2026 — AI & Privacy Law Resource</strong>
      <span>Citation • 500–700 word summary • doctrine • analysis • discussion • slides</span>
    </div>
    <nav aria-label="Page sections">
      <a href="#citation">Citation</a>
      <a href="#summary">Summary</a>
      <a href="#doctrine">Doctrinal Connections</a>
      <a href="#analysis">Critical Analysis</a>
      <a href="#question">Discussion Question</a>
      <a href="#slides">Slides</a>
    </nav>
  </div>
</header>

<main id="content">
  <div class="hero">
    <h1>[Artificial Intelligence and Privacy]</h1>
    <div class="meta">
      <span class="pill"><strong>Group:</strong> Group 1 / Riley Bone, John Bozell, Shalonte Branham, Zachary Broyles</span>
      <span class="pill"><strong>Date:</strong> February/17/2026</span>
      <span class="pill"><strong>Topic:</strong> AI governance & privacy regulation</span>
    </div>
    <p class="callout">
      <strong>Orientation:</strong> Author, Daniel Solove, argues AI is not a brand-new privacy category; it accelerates existing data problems
      to a scale where consent-based models break down, requiring structural oversight and institutional accountability.
    </p>
  </div>

  <div class="grid">
    <section class="card" id="citation">
      <h2>Full Citation</h2>
      <p class="subtle">The full article can be found linked below, or by searching the listed Bluebook citation.</p>

      <div class="label">Citation</div>
      <pre class="citation">Daniel J. Solove, “Artificial Intelligence and Privacy”, Florida Law Review 77([1]) ([2025]), 1-73, https://scholarship.law.ufl.edu/flr/vol77/iss1/1.</pre>

      <div class="divider"></div>
      <div class="label">Access & Metadata</div>
      <ul>
        <li><strong>Date published:</strong> [June 16, 2025]</li>
        <li><strong>Where accessed:</strong> Accessed through the University of Florida Law Scholarship Repository</li>
        <li><strong>Permanent link:</strong> <a href="https://scholarship.law.ufl.edu/flr/vol77/iss1/1" target="_blank" rel="noopener">https://https://scholarship.law.ufl.edu/flr/vol77/iss1/1</a></li>
      </ul>
    </section>

    <section class="card" id="quicktake">
      <h2>Quick Take</h2>
      <p class="subtle">One-screen capture of the article’s core claims.</p>

      <div class="label">Thesis (1–2 sentences)</div>
      <p>
        AI primarily amplifies long-standing privacy problems—collection, processing, and use of personal data—while its scale,
        speed, and inferential power render individual consent and control models increasingly ineffective.
      </p>

      <div class="divider"></div>
      <div class="label">Key implications</div>
      <ul>
        <li>Privacy governance should move from individual “notice-and-consent” toward organizational duties and structural oversight.</li>
        <li>Risk-tier frameworks can help, but raise implementation questions (who assesses risk, transparency, and general-purpose systems).</li>
        <li>Inference and automated decision systems intensify harms (surveillance, manipulation, bias) and complicate remedies.</li>
      </ul>
    </section>
  </div>

  <section class="card" id="summary" style="margin-top:16px;">
    <h2>Structured Summary (500–700 words)</h2>
    <p class="subtle">Solove articulates a framework for analyzing regulatory schemes for AI and highlights areas of concern with the use of AI.</p>

    <div class="label">1) Background / Context</div>
    <p>
      The article develops a framework for understanding how artificial intelligence reshapes privacy law by magnifying familiar
      weaknesses in existing regulatory models. Rather than treating AI as an entirely new legal category, the author emphasizes
      continuity: modern “AI” is largely machine learning that detects patterns in data, enabled by dramatic increases in data
      availability, computing power, and algorithmic sophistication. Because these systems depend on human choices—design,
      training objectives, curated datasets—the author cautions against anthropomorphizing AI in ways that obscure accountability.
      On this view, AI should be understood as the next step in algorithmic data analysis within a longer history of digital data
      practices. That framing matters because policymakers often respond incoherently when they regulate AI as categorically
      exceptional rather than as a set of intensified data-processing techniques.
    </p>

    <div class="label">2) Main Claim / Thesis</div>
    <p>
      The central claim is that AI intensifies long-standing privacy problems—especially those tied to collection, processing,
      and use—so severely that traditional privacy law, and particularly consent-based models, become increasingly ineffective.
      The author argues that frameworks centered on individual control assume people can meaningfully manage their privacy through
      notice and consent, but AI’s scale and inferential capacity make that assumption unrealistic. Even rights-forward regimes
      that provide access, correction, or deletion struggle because AI systems are built on collective datasets and complex models
      that individuals cannot fully understand or control. As AI blurs boundaries between collection, processing, and analysis,
      the law’s categorical triggers and remedial tools fail to track how privacy harms are actually produced.
    </p>

    <div class="label">3) Evidence & Reasoning</div>
    <p>
      The article advances a regulatory roadmap organized around categories of AI-driven privacy impact and compares existing
      approaches across jurisdictions. First, it critiques individual-control regulation (especially prominent in the United
      States) for over-relying on notice and consent. Next, it evaluates harm-based and risk-based approaches—illustrated by
      emerging EU models—arguing that risk-tier governance is more realistic because it focuses on potential harms rather than
      formal consent. Yet risk-based regulation raises difficult design questions: who conducts risk assessments, what level of
      transparency is required, and how to handle general-purpose AI systems that can be repurposed in unexpected contexts.
    </p>
    <p>
      The roadmap then highlights three technical domains that create distinctive privacy risks. (1) <strong>Data collection</strong>:
      AI drives enormous demand for data, often gathered through large-scale scraping or repurposed consumer data, undermining
      transparency, purpose limitation, and data minimization. Aggregation can transform publicly available information into
      invasive surveillance capabilities. (2) <strong>Data generation</strong>: AI can infer new personal information, effectively
      bypassing protections focused on direct collection, and can generate deceptive or harmful simulated content that increases
      risks of manipulation, fraud, and identity distortion. (3) <strong>Outcome influence</strong>: predictive and automated
      systems shape opportunities and constraints, potentially entrenching inequality through biased models, self-fulfilling
      expectations, and depersonalized decisions that ignore context. The author stresses that these issues are not solvable
      solely through technical “bias fixes,” but require governance interventions with enforceable standards.
    </p>

    <div class="label">4) Implications / Takeaways</div>
    <p>
      The article concludes that AI does not invent wholly new privacy problems; it exposes the inadequacy of privacy law built
      around individual data management. The proposed direction is a shift toward structural oversight, risk governance, and
      institutional accountability: substantive duties on organizations, stronger enforcement, stakeholder inclusion in system
      design, and remedies for systemic harms. Transparency and due process become central because complexity and “black box”
      operation make it difficult for individuals to contest automated outcomes. Effective AI governance, on this view, requires
      moving beyond consent toward systemic regulation that prevents harm and assigns responsibility at the institutional level.
    </p>
  </section>

  <div class="grid" style="margin-top:16px;">
    <section class="card" id="doctrine">
      <h2>Doctrinal Connections</h2>
      <p class="subtle">Swap/add items to match your course coverage. Each entry includes a “how it connects” note.</p>

      <div class="label">Statutes / Regulations / Rules</div>
      <ul>
        <li><strong>FTC Act § 5 (15 U.S.C. § 45)</strong> — Core U.S. consumer-privacy enforcement hook for unfair/deceptive data practices; relevant where AI systems rely on opaque collection or misleading disclosures.</li>
        <li><strong>Electronic Communications Privacy Act (ECPA)</strong> — Sets boundaries on interception/access; implicated by surveillance-like data capture and monitoring pipelines.</li>
        <li><strong>Illinois Biometric Information Privacy Act (BIPA)</strong> — Illustrates how biometric/face data used in ML can trigger specific duties (notice/consent/retention), and how inference changes stakes.</li>
        <li><strong>EU GDPR</strong> — Rights and duties framework that the article critiques as strained by collective datasets, inference, and model opacity.</li>
        <li><strong>EU AI Act</strong> — Exemplifies the risk-based approach the author sees as promising but administratively challenging.</li>
      </ul>

      <div class="divider"></div>

      <div class="label">Cases</div>
      <ul>
        <li><strong><em>Carpenter v. United States</em>, 138 S. Ct. 2206 (2018)</strong> — Recognizes heightened privacy interests in aggregated digital data; supports the article’s point that scale/aggregation changes the privacy calculus.</li>
        <li><strong><em>Spokeo, Inc. v. Robins</em>, 578 U.S. 330 (2016)</strong> and <strong><em>TransUnion LLC v. Ramirez</em>, 594 U.S. 413 (2021)</strong> — Standing limits for statutory privacy harms; relevant to remedies for systemic AI harms.</li>
        <li><strong><em>Riley v. California</em>, 573 U.S. 373 (2014)</strong> — Digital volume and sensitivity affect legal treatment; aligns with the article’s “scale intensifies” theme.</li>
        <li><strong><em>Jones v. Microsoft Corp.</em> (BIPA litigation line)</strong> — (Replace with your chosen BIPA/face-recognition case) illustrates consent-based duties colliding with ML training and deployment.</li>
      </ul>

      <div class="callout">
        <strong>Doctrinal map:</strong> The article’s critique fits U.S. privacy law’s reliance on notice/consent and ex post enforcement,
        while pointing toward risk-tier governance and duty-based regulation (organizational obligations, audits, impact assessments,
        and meaningful contestability for automated decisions).
      </div>
    </section>

    <section class="card" id="analysis">
      <h2>Critical Analysis</h2>
      <p class="subtle">Strengths and weaknesses, tied to governance design and enforceability.</p>

      <div class="label">Strengths</div>
      <ul>
        <li><strong>Conceptual clarity</strong> — Treating AI as “amplified data practice” avoids hype and keeps human institutions accountable for design and deployment choices.</li>
        <li><strong>Regulatory realism</strong> — The critique of consent-based privacy fits how people actually interact with notice regimes and how opaque ML systems function in practice.</li>
        <li><strong>Actionable roadmap</strong> — Organizing privacy impact by collection, generation (inference), and outcome influence is a useful way to translate technical systems into legal oversight targets.</li>
      </ul>

      <div class="divider"></div>

      <div class="label">Weaknesses / Limitations</div>
      <ul>
        <li><strong>Operational detail gap</strong> — Structural oversight and risk governance are persuasive in theory, but the hardest questions are administrative: who sets metrics, who audits, what transparency is required, and how regulators scale oversight.</li>
        <li><strong>General-purpose AI problem</strong> — The roadmap flags it, but governance for multipurpose models can become either too broad (stifling) or too narrow (easily evaded through “downstream” deployment).</li>
        <li><strong>Remedy/standing friction</strong> — In U.S. litigation, systemic harms often run into causation and standing barriers; stronger governance may require legislative change, not just doctrinal refinement.</li>
      </ul>

      <div class="divider"></div>

      <div class="label">Bottom line</div>
      <p>
        The article’s strongest contribution is reframing AI privacy as a scaling problem that breaks individual control models.
        Its proposed direction—duty-based, risk-oriented governance—sounds right, but success depends on concrete institutional
        design: enforceable standards, audit capacity, and remedies that reach systemic harms rather than isolated individual violations.
      </p>
    </section>
  </div>

  <section class="card" id="question" style="margin-top:16px;">
    <h2>Discussion Question</h2>
    <p class="subtle">One question designed to force doctrinal application and normative judgment.</p>
    <div class="callout">
      <strong>Question:</strong> In a duty-based risk governance privacy scheme for AI regulation, what should
      be the minimum enforceable obligations for organizations (e.g., impact assessments, auditability, contestability of decisions),
      and who should have standing to enforce them when harms are diffuse or primarily collective?
    </div>
  </section>

  <section class="card" id="slides" style="margin-top:16px;">
    <h2>Slides (Embedded or Linked)</h2>
    <p class="subtle">Paste your link below. For Google Slides, use “Publish to the web” to get an embeddable URL.</p>

    <div class="label">Link</div>
    <p><a href="[https://your-slides-link]" target="_blank" rel="noopener">Open slides in a new tab</a></p>

    <div class="divider"></div>

    <div class="label">Embedded slides</div>
    <div class="slides" role="region" aria-label="Embedded slides">
      <iframe
        src="[https://docs.google.com/presentation/d/e/XXXXXXXX/embed?start=false&loop=false&delayms=3000]"
        allowfullscreen="true"
        title="Slides Embed">
      </iframe>
    </div>
  </section>
</main>

<footer>
  <strong>Note:</strong> Replace placeholders in brackets with your article’s citation, your selected authorities, and your slide link.
</footer>
</body>
</html>
